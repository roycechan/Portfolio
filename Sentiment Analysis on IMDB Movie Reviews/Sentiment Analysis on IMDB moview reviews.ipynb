{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on IMDB moview review dataset\n",
    "Poorly rated movies are labeled 0 and highly rated movies are labeled 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:07.198701Z",
     "start_time": "2019-02-27T18:46:07.194711Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:07.404206Z",
     "start_time": "2019-02-27T18:46:07.363300Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv', encoding='iso8859')\n",
    "df_test = pd.read_csv('test.csv', encoding='iso8859')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:07.533902Z",
     "start_time": "2019-02-27T18:46:07.527951Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.columns = ['id','sentiment','review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:07.690659Z",
     "start_time": "2019-02-27T18:46:07.678664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:07.846103Z",
     "start_time": "2019-02-27T18:46:07.842114Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_list = df_train['review'].tolist()\n",
    "x_test_list = df_test['review'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:09.254387Z",
     "start_time": "2019-02-27T18:46:09.248402Z"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = open('stopwords.txt', ).read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:45.719318Z",
     "start_time": "2019-02-27T18:46:10.035541Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "\n",
    "nlp = spacy.load(\"en\", disable=['tagger', 'ner'])\n",
    "\n",
    "parsed_text_train = []\n",
    "parsed_text_test = []\n",
    "\n",
    "for i in range(len(x_train_list)):\n",
    "    for c in string.punctuation:\n",
    "        text = x_train_list[i].replace(c, \"\") \n",
    "    text = text.lower()\n",
    "    parsed_text_train.append(nlp(text))\n",
    "\n",
    "for i in range(len(x_test_list)):\n",
    "    for c in string.punctuation:\n",
    "        text = x_test_list[i].replace(c, \"\") \n",
    "    text = text.lower()\n",
    "    parsed_text_test.append(nlp(text))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:47.251028Z",
     "start_time": "2019-02-27T18:46:45.721083Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsed_text_train_filtered_max 504\n",
      "parsed_text_test_filtered_max 456\n"
     ]
    }
   ],
   "source": [
    "parsed_text_train_filtered = []\n",
    "parsed_text_test_filtered = []\n",
    "parsed_text_train_filtered_max = 0\n",
    "parsed_text_test_filtered_max = 0\n",
    "\n",
    "for i in range(len(parsed_text_train)):\n",
    "    inner_list = []\n",
    "    for token in parsed_text_train[i]:    \n",
    "        if (str(token).lower() not in stopwords) & (token.is_alpha or token.is_digit):\n",
    "            inner_list.append(str(token).lower())\n",
    "    parsed_text_train_filtered.append(inner_list)\n",
    "    if len(inner_list) > parsed_text_train_filtered_max:\n",
    "        parsed_text_train_filtered_max = len(inner_list)\n",
    "        \n",
    "    \n",
    "for i in range(len(parsed_text_test)):\n",
    "    inner_list = []\n",
    "    for token in parsed_text_test[i]:    \n",
    "        if (str(token).lower() not in stopwords) & (token.is_alpha or token.is_digit):\n",
    "            inner_list.append(str(token).lower())\n",
    "    parsed_text_test_filtered.append(inner_list)    \n",
    "    if len(inner_list) > parsed_text_test_filtered_max:\n",
    "        parsed_text_test_filtered_max = len(inner_list)\n",
    "\n",
    "print(\"parsed_text_train_filtered_max\",parsed_text_train_filtered_max)  \n",
    "print(\"parsed_text_test_filtered_max\",parsed_text_test_filtered_max)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:47.274959Z",
     "start_time": "2019-02-27T18:46:47.251989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review  0\n",
      "['stuff', 'going', 'moment', 'mj', 'started', 'listening', 'music', 'watching', 'odd', 'documentary', 'watched', 'wiz', 'watched', 'moonwalker', 'maybe', 'want', 'certain', 'insight', 'guy', 'thought', 'really', 'cool', 'eighties', 'maybe', 'make', 'mind', 'guilty', 'innocent', 'moonwalker', 'biography', 'feature', 'film', 'remember', 'going', 'cinema', 'originally', 'released', 'subtle', 'messages', 'mj', 'feeling', 'press', 'obvious', 'message', 'drugs', 'bad', 'impressive', 'course', 'michael', 'jackson', 'unless', 'remotely', 'like', 'mj', 'going', 'hate', 'boring', 'mj', 'egotist', 'consenting', 'making', 'movie', 'mj', 'fans', 'say', 'fans', 'true', 'really', 'nice', 'actual', 'feature', 'film', 'bit', 'finally', 'starts', '20', 'minutes', 'excluding', 'smooth', 'criminal', 'sequence', 'joe', 'pesci', 'convincing', 'psychopathic', 'powerful', 'drug', 'lord', 'wants', 'mj', 'dead', 'bad', 'mj', 'overheard', 'plans', 'nah', 'joe', 'pesci', 'character', 'ranted', 'wanted', 'people', 'know', 'supplying', 'drugs', 'dunno', 'maybe', 'hates', 'mj', 'cool', 'things', 'like', 'mj', 'turning', 'car', 'robot', 'speed', 'demon', 'sequence', 'director', 'patience', 'saint', 'came', 'filming', 'kiddy', 'bad', 'sequence', 'usually', 'directors', 'hate', 'working', 'kid', 'let', 'bunch', 'performing', 'complex', 'dance', 'line', 'movie', 'people', 'like', 'mj', 'level', 'think', 'people', 'stay', 'away', 'try', 'wholesome', 'message', 'ironically', 'mj', 'bestest', 'buddy', 'movie', 'girl', 'michael', 'jackson', 'truly', 'talented', 'people', 'grace', 'planet', 'guilty', 'attention', 'gave', 'subject', 'hmmm', 'know', 'people', 'different', 'closed', 'doors', 'know', 'fact', 'extremely', 'nice', 'stupid', 'guy', 'sickest', 'liars', 'hope']  \n",
      "\n",
      "Review  1\n",
      "['classic', 'war', 'timothy', 'hines', 'entertaining', 'film', 'obviously', 'goes', 'great', 'effort', 'lengths', 'faithfully', 'recreate', 'wells', 'classic', 'book', 'mr', 'hines', 'succeeds', 'watched', 'film', 'appreciated', 'fact', 'standard', 'predictable', 'hollywood', 'fare', 'comes', 'year', 'spielberg', 'version', 'tom', 'cruise', 'slightest', 'resemblance', 'book', 'obviously', 'looks', 'different', 'things', 'movie', 'envision', 'amateur', 'look', 'criticize', 'rate', 'movie', 'important', 'bases', 'like', 'entertained', 'people', 'agree', 'enjoyed', 'effort', 'mr', 'hines', 'faithful', 'wells', 'classic', 'novel', 'entertaining', 'easy', 'overlook', 'perceive', 'shortcomings']  \n",
      "\n",
      "Review  2\n",
      "['film', 'starts', 'manager', 'nicholas', 'bell', 'giving', 'welcome', 'investors', 'robert', 'carradine', 'primal', 'park', 'secret', 'project', 'mutating', 'primal', 'animal', 'using', 'fossilized', 'dna', 'like', 'jurassik', 'park', 'scientists', 'resurrect', 'nature', 'fearsome', 'predators', 'sabretooth', 'tiger', 'smilodon', 'scientific', 'ambition', 'turns', 'deadly', 'high', 'voltage', 'fence', 'opened', 'creature', 'escape', 'begins', 'savagely', 'stalking', 'prey', 'human', 'visitors', 'tourists', 'youngsters', 'enter', 'restricted', 'area', 'security', 'center', 'attacked', 'pack', 'large', 'pre', 'historical', 'animals', 'deadlier', 'bigger', 'addition', 'security', 'agent', 'stacy', 'haiduk', 'mate', 'brian', 'wimmer', 'fight', 'hardly', 'carnivorous', 'smilodons', 'sabretooths', 'course', 'real', 'star', 'stars', 'astounding', 'terrifyingly', 'convincing', 'giant', 'animals', 'savagely', 'stalking', 'prey', 'group', 'run', 'afoul', 'fight', 'nature', 'fearsome', 'predators', 'furthermore', 'sabretooth', 'dangerous', 'slow', 'stalks', 'movie', 'delivers', 'goods', 'lots', 'blood', 'gore', 'beheading', 'hair', 'raising', 'chills', 'scares', 'sabretooths', 'appear', 'mediocre', 'special', 'story', 'provides', 'exciting', 'stirring', 'entertainment', 'results', 'quite', 'boring', 'giant', 'animals', 'majority', 'generator', 'totally', 'lousy', 'performances', 'players', 'reacting', 'appropriately', 'vigorously', 'physical', 'performances', 'dodging', 'beasts', 'running', 'bound', 'leaps', 'dangling', 'walls', 'packs', 'ridiculous', 'final', 'deadly', 'scene', 'small', 'kids', 'realistic', 'gory', 'violent', 'attack', 'scenes', 'films', 'sabretooths', 'smilodon', 'following', 'james', 'r', 'hickox', 'vanessa', 'angel', 'david', 'keith', 'john', 'rhys', 'davies', 'better', 'roland', 'emmerich', 'steven', 'strait', 'cliff', 'curtis', 'camilla', 'belle', 'motion', 'picture', 'filled', 'bloody', 'moments', 'badly', 'directed', 'george', 'miller', 'originality', 'takes', 'elements', 'previous', 'films', 'miller', 'australian', 'director', 'usually', 'working', 'television', 'tidal', 'wave', 'journey', 'center', 'earth', 'occasionally', 'cinema', 'man', 'snowy', 'river', 'zeus', 'roxanne', 'robinson', 'crusoe', 'rating', 'average', 'barrel']  \n",
      "\n",
      "Review  3\n",
      "['assumed', 'praised', 'film', 'greatest', 'filmed', 'opera', 'read', 'care', 'opera', 'care', 'wagner', 'care', 'desire', 'appear', 'cultured', 'representation', 'wagner', 'swan', 'song', 'movie', 'strikes', 'unmitigated', 'disaster', 'leaden', 'reading', 'score', 'matched', 'tricksy', 'lugubrious', 'realisation', 'questionable', 'people', 'ideas', 'opera', 'matter', 'play', 'especially', 'shakespeare', 'allowed', 'near', 'theatre', 'film', 'studio', 'syberberg', 'fashionably', 'smallest', 'justification', 'wagner', 'text', 'decided', 'parsifal', 'bisexual', 'integration', 'title', 'character', 'stages', 'transmutes', 'kind', 'beatnik', 'babe', 'continues', 'sing', 'high', 'tenor', 'actors', 'film', 'singers', 'double', 'dose', 'armin', 'jordan', 'conductor', 'seen', 'face', 'heard', 'voice', 'amfortas', 'appears', 'monstrously', 'double', 'exposure', 'kind', 'batonzilla', 'conductor', 'ate', 'monsalvat', 'playing', 'good', 'friday', 'music', 'way', 'transcendant', 'loveliness', 'nature', 'represented', 'scattering', 'shopworn', 'flaccid', 'crocuses', 'stuck', 'ill', 'laid', 'turf', 'expedient', 'baffles', 'theatre', 'piece', 'imperfections', 'thoughts', 'ca', 'think', 'syberberg', 'splice', 'parsifal', 'gurnemanz', 'mountain', 'pasture', 'lush', 'provided', 'julie', 'andrews', 'sound', 'music', 'sound', 'hard', 'endure', 'high', 'voices', 'trumpets', 'particular', 'possessing', 'aural', 'glare', 'adds', 'sort', 'fatigue', 'impatience', 'uninspired', 'conducting', 'paralytic', 'unfolding', 'ritual', 'review', 'mentioned', '1951', 'bayreuth', 'recording', 'knappertsbusch', 'tempi', 'slow', 'jordan', 'altogether', 'lacks', 'sense', 'pulse', 'feeling', 'ebb', 'flow', 'music', 'half', 'century', 'orchestral', 'sound', 'set', 'modern', 'pressings', 'superior', 'film']  \n",
      "\n",
      "Review  4\n",
      "['superbly', 'trashy', 'wondrously', 'unpretentious', '80', 'exploitation', 'hooray', 'pre', 'credits', 'opening', 'sequences', 'somewhat', 'false', 'impression', 'dealing', 'harrowing', 'drama', 'need', 'fear', 'barely', 'minutes', 'later', 'necks', 'nonsensical', 'chainsaw', 'battles', 'rough', 'fist', 'fights', 'lurid', 'dialogs', 'gratuitous', 'nudity', 'bo', 'ingrid', 'orphaned', 'siblings', 'unusually', 'close', 'slightly', 'perverted', 'relationship', 'imagine', 'playfully', 'ripping', 'towel', 'covers', 'sister', 'naked', 'body', 'stare', 'unshaven', 'genitals', 'minutes', 'bo', 'sister', 'judging', 'dubbed', 'laughter', 'mind', 'sick', 'dude', 'kids', 'fled', 'russia', 'parents', 'nasty', 'soldiers', 'brutally', 'slaughtered', 'mommy', 'daddy', 'friendly', 'smuggler', 'took', 'custody', 'raised', 'trained', 'bo', 'ingrid', 'expert', 'smugglers', 'actual', 'plot', 'lifts', '20', 'years', 'later', 'facing', 'ultimate', 'quest', 'mythical', 'incredibly', 'valuable', 'white', 'diamond', 'coincidentally', 'things', 'life', 'little', 'sense', 'plot', 'narrative', 'structure', 'sure', 'lot', 'fun', 'watch', 'time', 'clue', 'beating', 'cause', 'bet', 'actors', 'understood', 'violence', 'magnificently', 'grotesque', 'single', 'plot', 'twist', 'pleasingly', 'retarded', 'script', 'goes', 'totally', 'bonkers', 'repair', 'suddenly', 'wo', 'reveal', 'reason', 'bo', 'needs', 'replacement', 'ingrid', 'fred', 'williamson', 'enters', 'scene', 'big', 'cigar', 'mouth', 'sleazy', 'black', 'fingers', 'local', 'prostitutes', 'bo', 'principal', 'opponent', 'italian', 'chick', 'big', 'breasts', 'hideous', 'accent', 'preposterous', 'catchy', 'theme', 'song', 'plays', 'dozen', 'times', 'film', 'obligatory', 'falling', 'montage', 'loads', 'attractions', 'god', 'brilliant', 'experience', 'original', 'french', 'title', 'translates', 'uniquely', 'appropriate', 'makes', 'sense', 'rest', 'movie']  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx,i in enumerate(parsed_text_train_filtered[:5]):\n",
    "    print(\"Review \",idx)\n",
    "    print(i,\" \", end='')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn movie reviews into vectors with pre-computed GloVe word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T18:46:54.023945Z",
     "start_time": "2019-02-27T18:46:47.276922Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "word_to_embedding = {}\n",
    "\n",
    "# we will use the 50-dimensional embedding vectors\n",
    "with open(\"./glove.6B.50d.txt\", encoding='UTF-8') as f:\n",
    "    # each row represents a word vector\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # the first part is word\n",
    "        word = values[0]\n",
    "        # the rest of the values form the embedding vector\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        word_to_embedding[word] = embedding\n",
    "\n",
    "print('Found %s word vectors.' % len(word_to_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T19:18:25.776206Z",
     "start_time": "2019-02-27T19:18:25.535834Z"
    }
   },
   "outputs": [],
   "source": [
    "# i-th movie review should have an embedding given by the i-th row of the 2D array\n",
    "embedding_dim = 50\n",
    "vocab_size = len(parsed_text_train_filtered)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for review_idx in range(vocab_size):\n",
    "    words_found_in_embedding = 0\n",
    "    for word_idx in range(len(parsed_text_train_filtered[review_idx])):\n",
    "        if parsed_text_train_filtered[review_idx][word_idx] in word_to_embedding:\n",
    "            word = parsed_text_train_filtered[review_idx][word_idx] \n",
    "            embedding_matrix[review_idx] += word_to_embedding[word]\n",
    "            words_found_in_embedding += 1\n",
    "    embedding_matrix[review_idx] /= words_found_in_embedding\n",
    "\n",
    "train_x = embedding_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T19:18:23.924449Z",
     "start_time": "2019-02-27T19:18:23.857200Z"
    }
   },
   "outputs": [],
   "source": [
    "# do the same for test set\n",
    "vocab_size = len(parsed_text_test_filtered)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for review_idx in range(vocab_size):\n",
    "    words_found_in_embedding = 0\n",
    "    for word_idx in range(len(parsed_text_test_filtered[review_idx])):\n",
    "        if parsed_text_test_filtered[review_idx][word_idx] in word_to_embedding:\n",
    "            word = parsed_text_test_filtered[review_idx][word_idx] \n",
    "            embedding_matrix[review_idx] += word_to_embedding[word]\n",
    "            words_found_in_embedding += 1\n",
    "    embedding_matrix[review_idx] /= words_found_in_embedding\n",
    "\n",
    "test_x = embedding_matrix      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T19:22:10.762452Z",
     "start_time": "2019-02-27T19:22:10.758464Z"
    }
   },
   "outputs": [],
   "source": [
    "# create train and test labels\n",
    "train_y = df_train['sentiment'].values\n",
    "test_y = df_test['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a polynomial kernel SVM using the review embeddings as feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T19:34:52.105456Z",
     "start_time": "2019-02-27T19:34:50.241708Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.0001 / d: 1 0.545\n",
      "C: 0.0001 / d: 1 0.55\n",
      "C: 0.0001 / d: 1 0.47\n",
      "C: 0.0001 / d: 1 0.51\n",
      "C: 0.0001 / d: 1 0.5125628140703518\n",
      "MEAN C: 0.0001 / d: 1 0.5175125628140703\n",
      "C: 0.0001 / d: 2 0.545\n",
      "C: 0.0001 / d: 2 0.55\n",
      "C: 0.0001 / d: 2 0.47\n",
      "C: 0.0001 / d: 2 0.51\n",
      "C: 0.0001 / d: 2 0.5125628140703518\n",
      "MEAN C: 0.0001 / d: 2 0.5175125628140703\n",
      "C: 0.0001 / d: 3 0.545\n",
      "C: 0.0001 / d: 3 0.55\n",
      "C: 0.0001 / d: 3 0.47\n",
      "C: 0.0001 / d: 3 0.51\n",
      "C: 0.0001 / d: 3 0.5125628140703518\n",
      "MEAN C: 0.0001 / d: 3 0.5175125628140703\n",
      "C: 0.1 / d: 1 0.545\n",
      "C: 0.1 / d: 1 0.55\n",
      "C: 0.1 / d: 1 0.47\n",
      "C: 0.1 / d: 1 0.51\n",
      "C: 0.1 / d: 1 0.5125628140703518\n",
      "MEAN C: 0.1 / d: 1 0.5175125628140703\n",
      "C: 0.1 / d: 2 0.545\n",
      "C: 0.1 / d: 2 0.55\n",
      "C: 0.1 / d: 2 0.47\n",
      "C: 0.1 / d: 2 0.51\n",
      "C: 0.1 / d: 2 0.5125628140703518\n",
      "MEAN C: 0.1 / d: 2 0.5175125628140703\n",
      "C: 0.1 / d: 3 0.545\n",
      "C: 0.1 / d: 3 0.55\n",
      "C: 0.1 / d: 3 0.47\n",
      "C: 0.1 / d: 3 0.51\n",
      "C: 0.1 / d: 3 0.5125628140703518\n",
      "MEAN C: 0.1 / d: 3 0.5175125628140703\n",
      "C: 100.0 / d: 1 0.765\n",
      "C: 100.0 / d: 1 0.73\n",
      "C: 100.0 / d: 1 0.715\n",
      "C: 100.0 / d: 1 0.745\n",
      "C: 100.0 / d: 1 0.7437185929648241\n",
      "MEAN C: 100.0 / d: 1 0.7397437185929648\n",
      "C: 100.0 / d: 2 0.735\n",
      "C: 100.0 / d: 2 0.73\n",
      "C: 100.0 / d: 2 0.715\n",
      "C: 100.0 / d: 2 0.74\n",
      "C: 100.0 / d: 2 0.7437185929648241\n",
      "MEAN C: 100.0 / d: 2 0.7327437185929648\n",
      "C: 100.0 / d: 3 0.71\n",
      "C: 100.0 / d: 3 0.71\n",
      "C: 100.0 / d: 3 0.71\n",
      "C: 100.0 / d: 3 0.735\n",
      "C: 100.0 / d: 3 0.7185929648241206\n",
      "MEAN C: 100.0 / d: 3 0.7167185929648241\n",
      "100.0 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "num_folds = 5\n",
    "k_fold = KFold(num_folds)\n",
    "C_values = np.logspace(-4, 2, 3)\n",
    "D_values = range(1, 4)\n",
    "\n",
    "indices = range(train_x.shape[0])\n",
    "\n",
    "arg_max = None\n",
    "max_cross_val_score = -np.inf\n",
    "for C in C_values:\n",
    "    for d in D_values:\n",
    "        fold_scores = []\n",
    "        for train_indices, val_indices in k_fold.split(indices):\n",
    "            classifier = SVC(kernel='poly',C=C,degree=d,gamma='auto')\n",
    "            classifier.fit(train_x[train_indices], train_y[train_indices])\n",
    "            train_y_pred = classifier.predict(train_x[val_indices])\n",
    "            fold_score = accuracy_score(train_y[val_indices], train_y_pred)\n",
    "            fold_scores.append(fold_score)\n",
    "            print('C:', C, '/ d:', d, fold_score)\n",
    "\n",
    "        cross_val_score = np.mean(fold_scores)\n",
    "        print('MEAN C:', C, '/ d:', d, cross_val_score)\n",
    "        if cross_val_score > max_cross_val_score:\n",
    "            max_cross_val_score = cross_val_score\n",
    "            arg_max = (C, d)\n",
    "            \n",
    "best_C, best_d = arg_max\n",
    "print(best_C, best_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T19:36:11.135867Z",
     "start_time": "2019-02-27T19:36:11.092982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=1, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svm = SVC(kernel='poly', gamma='auto', C=best_C, degree=best_d)\n",
    "model_svm.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T19:38:25.956610Z",
     "start_time": "2019-02-27T19:38:25.941652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7429718875502008\n"
     ]
    }
   ],
   "source": [
    "test_y_predict = model_svm.predict(test_x)\n",
    "print(\"Accuracy:\",accuracy_score(test_y_predict,test_y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
